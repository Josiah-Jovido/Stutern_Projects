{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressor Model for Potential Ship Buyers (Predicting Crew Size)\n",
    "The objective of this model is to recommend the crew member size for potential ship buyers. The dataset used for the model can be found in the repository.\n",
    "#### Checklist\n",
    "* Read the file and display columns.\n",
    "* Calculate basic statistics of the data (count, mean, std, etc), examine data and state observations.\n",
    "* Select columns that will be probably important to predict crew size.\n",
    "* Create training and testing sets (use 60% of the data for the training and reminder for testing).\n",
    "* Build a machine learning model to predict the crew size.\n",
    "* Calculate the Pearson correlation coefficient for the training set and testing data sets.\n",
    "* Explain Overfitting, and how it can avoided. \n",
    "* What’s the difference between bias and variance?\n",
    "* When Will You Use Classification over Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_data = pd.read_csv('ship_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ship_name</th>\n",
       "      <th>Cruise_line</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tonnage</th>\n",
       "      <th>passengers</th>\n",
       "      <th>length</th>\n",
       "      <th>cabins</th>\n",
       "      <th>passenger_density</th>\n",
       "      <th>crew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Journey</td>\n",
       "      <td>Azamara</td>\n",
       "      <td>6</td>\n",
       "      <td>30.277</td>\n",
       "      <td>6.94</td>\n",
       "      <td>5.94</td>\n",
       "      <td>3.55</td>\n",
       "      <td>42.64</td>\n",
       "      <td>3.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quest</td>\n",
       "      <td>Azamara</td>\n",
       "      <td>6</td>\n",
       "      <td>30.277</td>\n",
       "      <td>6.94</td>\n",
       "      <td>5.94</td>\n",
       "      <td>3.55</td>\n",
       "      <td>42.64</td>\n",
       "      <td>3.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Celebration</td>\n",
       "      <td>Carnival</td>\n",
       "      <td>26</td>\n",
       "      <td>47.262</td>\n",
       "      <td>14.86</td>\n",
       "      <td>7.22</td>\n",
       "      <td>7.43</td>\n",
       "      <td>31.80</td>\n",
       "      <td>6.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conquest</td>\n",
       "      <td>Carnival</td>\n",
       "      <td>11</td>\n",
       "      <td>110.000</td>\n",
       "      <td>29.74</td>\n",
       "      <td>9.53</td>\n",
       "      <td>14.88</td>\n",
       "      <td>36.99</td>\n",
       "      <td>19.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Destiny</td>\n",
       "      <td>Carnival</td>\n",
       "      <td>17</td>\n",
       "      <td>101.353</td>\n",
       "      <td>26.42</td>\n",
       "      <td>8.92</td>\n",
       "      <td>13.21</td>\n",
       "      <td>38.36</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Ship_name Cruise_line  Age  Tonnage  passengers  length  cabins  \\\n",
       "0      Journey     Azamara    6   30.277        6.94    5.94    3.55   \n",
       "1        Quest     Azamara    6   30.277        6.94    5.94    3.55   \n",
       "2  Celebration    Carnival   26   47.262       14.86    7.22    7.43   \n",
       "3     Conquest    Carnival   11  110.000       29.74    9.53   14.88   \n",
       "4      Destiny    Carnival   17  101.353       26.42    8.92   13.21   \n",
       "\n",
       "   passenger_density   crew  \n",
       "0              42.64   3.55  \n",
       "1              42.64   3.55  \n",
       "2              31.80   6.70  \n",
       "3              36.99  19.10  \n",
       "4              38.36  10.00  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ship_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Tonnage</th>\n",
       "      <th>passengers</th>\n",
       "      <th>length</th>\n",
       "      <th>cabins</th>\n",
       "      <th>passenger_density</th>\n",
       "      <th>crew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>158.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>158.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15.689873</td>\n",
       "      <td>71.284671</td>\n",
       "      <td>18.457405</td>\n",
       "      <td>8.130633</td>\n",
       "      <td>8.830000</td>\n",
       "      <td>39.900949</td>\n",
       "      <td>7.794177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.615691</td>\n",
       "      <td>37.229540</td>\n",
       "      <td>9.677095</td>\n",
       "      <td>1.793474</td>\n",
       "      <td>4.471417</td>\n",
       "      <td>8.639217</td>\n",
       "      <td>3.503487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.329000</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>2.790000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>17.700000</td>\n",
       "      <td>0.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>46.013000</td>\n",
       "      <td>12.535000</td>\n",
       "      <td>7.100000</td>\n",
       "      <td>6.132500</td>\n",
       "      <td>34.570000</td>\n",
       "      <td>5.480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>14.000000</td>\n",
       "      <td>71.899000</td>\n",
       "      <td>19.500000</td>\n",
       "      <td>8.555000</td>\n",
       "      <td>9.570000</td>\n",
       "      <td>39.085000</td>\n",
       "      <td>8.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>90.772500</td>\n",
       "      <td>24.845000</td>\n",
       "      <td>9.510000</td>\n",
       "      <td>10.885000</td>\n",
       "      <td>44.185000</td>\n",
       "      <td>9.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>48.000000</td>\n",
       "      <td>220.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>11.820000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>71.430000</td>\n",
       "      <td>21.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Age     Tonnage  passengers      length      cabins  \\\n",
       "count  158.000000  158.000000  158.000000  158.000000  158.000000   \n",
       "mean    15.689873   71.284671   18.457405    8.130633    8.830000   \n",
       "std      7.615691   37.229540    9.677095    1.793474    4.471417   \n",
       "min      4.000000    2.329000    0.660000    2.790000    0.330000   \n",
       "25%     10.000000   46.013000   12.535000    7.100000    6.132500   \n",
       "50%     14.000000   71.899000   19.500000    8.555000    9.570000   \n",
       "75%     20.000000   90.772500   24.845000    9.510000   10.885000   \n",
       "max     48.000000  220.000000   54.000000   11.820000   27.000000   \n",
       "\n",
       "       passenger_density        crew  \n",
       "count         158.000000  158.000000  \n",
       "mean           39.900949    7.794177  \n",
       "std             8.639217    3.503487  \n",
       "min            17.700000    0.590000  \n",
       "25%            34.570000    5.480000  \n",
       "50%            39.085000    8.150000  \n",
       "75%            44.185000    9.990000  \n",
       "max            71.430000   21.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ship_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 158 entries, 0 to 157\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Ship_name          158 non-null    object \n",
      " 1   Cruise_line        158 non-null    object \n",
      " 2   Age                158 non-null    int64  \n",
      " 3   Tonnage            158 non-null    float64\n",
      " 4   passengers         158 non-null    float64\n",
      " 5   length             158 non-null    float64\n",
      " 6   cabins             158 non-null    float64\n",
      " 7   passenger_density  158 non-null    float64\n",
      " 8   crew               158 non-null    float64\n",
      "dtypes: float64(6), int64(1), object(2)\n",
      "memory usage: 11.2+ KB\n"
     ]
    }
   ],
   "source": [
    "ship_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ship_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From analyzing the dataset we can see that the dataset has two object data type columns, one integer dtype and six float dtype, there are no missing values present, therefore data cleaning/handling of missing values is not needed, we can also see that the dataset has 158 columns and 9 rows with the crew column our target(dependent variable). The dataset also contains alot of continuous values therefore a regressor model will be best for fitting and predicting the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the data\n",
    "There is still some minor but essential data preprocessing needed before we proceed towards building our machine learning model. From observation we can see that the dataset has some non-numeric values (dtype object columns), these non-numeric columns needs to be handled. First, we will be converting all the non-numeric values into numeric ones. We do this because not only it results in a faster computation but also many machine learning models (like XGBoost) (and especially the ones developed using scikit-learn) require the data to be in a strictly numeric format. We will do this by using a technique called label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Instantiate LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Iterate over all the values of each column and extract their dtypes\n",
    "for col in ship_data.columns.values:\n",
    "    # Compare if the dtype is object\n",
    "    if ship_data[col].dtypes=='object':\n",
    "    # Use LabelEncoder to do the numeric transformation\n",
    "        ship_data[col]=le.fit_transform(ship_data[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data set into train and test sets and feature selection\n",
    "I have successfully converted all the non-numeric values to numeric ones.\n",
    "\n",
    "Now, i will split the data into train set and test set to prepare the data for two different phases of machine learning modeling: training and testing. Ideally, no information from the test data should be used to scale the training data or should be used to direct the training process of a machine learning model. Hence, i will first split the data and then apply the scaling.\n",
    "\n",
    "Also, features like Ship_name and Cruise_line are not as important as the other features in the dataset for predicting credit card approvals. I will drop them to design the machine learning model with the best set of features. In Data Science literature, this is often referred to as feature selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age  Tonnage  passengers  length  cabins  passenger_density\n",
      "0    6   30.277        6.94    5.94    3.55              42.64\n",
      "1    6   30.277        6.94    5.94    3.55              42.64\n",
      "2   26   47.262       14.86    7.22    7.43              31.80\n",
      "3   11  110.000       29.74    9.53   14.88              36.99\n",
      "4   17  101.353       26.42    8.92   13.21              38.36\n"
     ]
    }
   ],
   "source": [
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Drop the features Ship_name and Cruise_line\n",
    "ship_data = ship_data.drop(['Ship_name', 'Cruise_line'], axis=1)\n",
    "#ship_data = ship_data.values\n",
    "\n",
    "# Segregate features and target into separate variables\n",
    "X = ship_data.drop('crew', axis=1)\n",
    "y = ship_data['crew']\n",
    "\n",
    "print(X.head())\n",
    "y.head()\n",
    "\n",
    "# convert the DataFrame to a NumPy array\n",
    "X = X.values\n",
    "y = y.values\n",
    "\n",
    "# Split into 60% train set and 40% test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the data ii\n",
    "The data is now split into two separate sets - train and test sets respectively. We are only left with one final preprocessing step of scaling before we can fit a machine learning model to the data. After considerable analysis of the dataset i decided to use Standardization as my rescaling method because its not limited in a range unlike Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Instantiate StandardScaler and use it to rescale X_train and X_test\n",
    "SS_scaler = StandardScaler()\n",
    "rescaledX_train = SS_scaler.fit_transform(X_train)\n",
    "rescaledX_test = SS_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building my machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2: 0.9362030498069416\n",
      "Root Mean Squared Error: 0.8977962325945485\n"
     ]
    }
   ],
   "source": [
    "# Import LinearRegression, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create the regressor: reg_all\n",
    "reg_all = LinearRegression()\n",
    "\n",
    "# Fit logreg to the train set\n",
    "reg_all.fit(rescaledX_train, y_train)\n",
    "\n",
    "# Predict on the test data: y_pred\n",
    "y_pred = reg_all.predict(rescaledX_test)\n",
    "\n",
    "# Compute and print R^2 and RMSE\n",
    "print(\"R^2: {}\".format(reg_all.score(rescaledX_test, y_test)))\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"Root Mean Squared Error: {}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the values gotten above we can see that the model R^2 value which can also be defined as the accuracy of the model is 0.9362030498069416 which is a very good model as the score is close to 1, also i tested the model on a separate test data(y_test) and used root mean squared to calculate the efficiency of the model predicting on new data, as we can see from the result gotten the model has passed all the tests in flying colors. We can confidently use this model for predicting the crew member size for ship buyers.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pearson correlation coefficient for the training and testing data sets.\n",
    "Correlation is a technique for investigating the relationship between two quantitative, continuous variables, for example, Age and Tonnage. Pearson's correlation coefficient (r) is a measure of the strength of the association between the two variables.\n",
    "##### Pearson correlation formula\n",
    "![image](https://media.geeksforgeeks.org/wp-content/uploads/20200311233526/formula6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Coefficients: [-0.00963213  0.54389059 -1.68304167  0.54554151  3.88951399  0.05898596]\n"
     ]
    }
   ],
   "source": [
    "print('Pearson Coefficients:', reg_all.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is overfitting?\n",
    "Overfitting occurs when your model learns too much from training data and isn’t able to generalize the underlying information. When this happens, the model is able to describe training data very accurately but loses precision on every dataset it has not been trained on. This is completely bad because we want our model to be reasonably good on data that it has never seen before.\n",
    "#### Why does it happen?\n",
    "In machine learning, simplicity is the key. We want to generalize the information obtained from the training dataset, so we can surely say that we run the risk of overfitting if we use complex models.\n",
    "Complex models will likely over-learn from training data and will think that the random error that drifts training data from the underlying dynamics is actually worth learning from. That’s the exact point at which the model stops generalizing and starts overfitting.\n",
    "Complexity is often measured with the number of parameters used by your model during it’s learning procedure. For example, the number of parameters in linear regression, the number of neurons in a neural network, and so on.\n",
    "So, the lower the number of the parameters, the higher the simplicity and, reasonably, the lower the risk of overfitting.\n",
    "#### A simple example of overfitting\n",
    "![image](https://miro.medium.com/proxy/1*1z_Id7wNBoGVWnWl238wmg.png)\n",
    "\n",
    "Now it’s clear what happens here. The polynomial fits training data perfectly but loses precision on the test set. It doesn’t even get close to test points.\n",
    "#### How to avoid overfitting\n",
    "* Cross validation: Cross-validation is a powerful preventative measure against overfitting. The idea is clever: Use your initial training data to generate multiple mini train-test splits. Use these splits to tune your model. In standard k-fold cross-validation, we partition the data into k subsets, called folds. Then, we iteratively train the algorithm on k-1 folds while using the remaining fold as the test set (called the “holdout fold”). Cross-validation allows you to tune hyperparameters with only your original training set. This allows you to keep your test set as a truly unseen dataset for selecting your final model.\n",
    "* Training with more data: It won’t work every time, but training with more data can help algorithms detect the signal better. You should ensure your data is clean and relevant.\n",
    "* Remove Features: Some algorithms have built-in feature selection. For those that don’t, you can manually improve their generalizability by removing irrelevant input features. An interesting way to do so is to tell a story about how each feature fits into the model. This is like the data scientist's spin on software engineer’s rubber duck debugging technique, where they debug their code by explaining it, line-by-line, to a rubber duck. If anything doesn't make sense, or if it’s hard to justify certain features, this is a good way to identify them. In addition, there are several feature selection heuristics you can use for a good starting point.\n",
    "* Regularization: Regularization refers to a broad range of techniques for artificially forcing your model to be simpler. The method will depend on the type of learner you’re using. For example, you could prune a decision tree, use dropout on a neural network, or add a penalty parameter to the cost function in regression. Often times, the regularization method is a hyperparameter as well, which means it can be tuned through cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias and Variance\n",
    "The prediction error for any machine learning algorithm can be broken down into three parts:\n",
    "* Bias Error\n",
    "* Variance Error\n",
    "\n",
    "#### Bias Error\n",
    "Bias are the simplifying assumptions made by a model to make the target function easier to learn. Generally, linear algorithms have a high bias making them fast to learn and easier to understand but generally less flexible. In turn, they have lower predictive performance on complex problems that fail to meet the simplifying assumptions of the algorithms bias.\n",
    "  \n",
    "  * Low Bias: Suggests less assumptions about the form of the target function.\n",
    "  * High-Bias: Suggests more assumptions about the form of the target function.\n",
    "\n",
    "Examples of low-bias machine learning algorithms include: Decision Trees, k-Nearest Neighbors and Support Vector Machines. Examples of high-bias machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression.\n",
    "#### Variance Error\n",
    "Variance is the amount that the estimate of the target function will change if different training data was used. The target function is estimated from the training data by a machine learning algorithm, so we should expect the algorithm to have some variance. Ideally, it should not change too much from one training dataset to the next, meaning that the algorithm is good at picking out the hidden underlying mapping between the inputs and the output variables. Machine learning algorithms that have a high variance are strongly influenced by the specifics of the training data. This means that the specifics of the training have influences over the number and types of parameters used to characterize the mapping function.\n",
    "  \n",
    "  * Low Variance: Suggests small changes to the estimate of the target function with changes to the training dataset.\n",
    "  * High Variance: Suggests large changes to the estimate of the target function with changes to the training dataset.\n",
    "\n",
    "Generally, nonlinear machine learning algorithms that have a lot of flexibility have a high variance. For example, decision trees have a high variance, that is even higher if the trees are not pruned before use. Examples of low-variance machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and Logistic Regression. Examples of high-variance machine learning algorithms include: Decision Trees, k-Nearest Neighbors and Support Vector Machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use classification over regression\n",
    "Classification is used when the output variable is a category such as “red” or “blue”, “spam” or “not spam”. It is used to draw a conclusion from observed values. Differently from, regression which is used when the output variable is a real or continuous value like “age”, “salary”, etc. Which is why i used regression in building my model because the values in the dataset are continuous."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
